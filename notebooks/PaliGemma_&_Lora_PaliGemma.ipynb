{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#⚠️ Note: Outputs are cleared for GitHub compatibility.\n",
        "All experiments and results are reproducible by running the notebook cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PaliGemma Vision-Language Model for Medical VQA\n",
        "\n",
        "This notebook implements:\n",
        "- Zero-shot inference using PaliGemma\n",
        "- Evaluation on closed-ended and open-ended questions\n",
        "- LoRA fine-tuning experiments\n",
        "- Comparative analysis with CNN-GRU baseline\n",
        "- Qualitative analysis of generated answers\n",
        "\n",
        "Dataset: VQA-RAD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW9I-J4MbEHG",
        "outputId": "985b21e0-6604-4ef3-cbea-72557dd22dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: False\n",
            "GPU name: No GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnHJidWYbKFn"
      },
      "outputs": [],
      "source": [
        "#Mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQDdveqrbM3Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/WOA7015_MedVQA\"\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "print(\"Project directory:\", base_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxlF3q2JbRfl"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/VQA.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/WOA7015_MedVQA/data/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"ZIP extracted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BcXPUGNbYc3"
      },
      "outputs": [],
      "source": [
        "#Check dataset folder\n",
        "!ls /content/drive/MyDrive/WOA7015_MedVQA/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf8uo6-qbbfK"
      },
      "outputs": [],
      "source": [
        "#Load & Inspect the Dataset\n",
        "import os\n",
        "import json\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/WOA7015_MedVQA/data\"\n",
        "\n",
        "image_dir = os.path.join(base_path, \"VQA_RAD Image Folder\")\n",
        "json_path = os.path.join(base_path, \"VQA_RAD Dataset Public.json\")\n",
        "\n",
        "print(\"Image folder exists:\", os.path.exists(image_dir))\n",
        "print(\"JSON file exists:\", os.path.exists(json_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkG1kMEMbfbw"
      },
      "outputs": [],
      "source": [
        "#Dataset Structure and Load the JSON File\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Total QA samples:\", len(data))\n",
        "print(\"Sample entry:\")\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbzuRFnBbkNe"
      },
      "outputs": [],
      "source": [
        "#Inspect One Sample\n",
        "sample = data[0]\n",
        "\n",
        "print(\"Image:\", sample[\"image_name\"])\n",
        "print(\"Question:\", sample[\"question\"])\n",
        "print(\"Answer:\", sample[\"answer\"])\n",
        "print(\"Type:\", sample[\"question_type\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bAMv8B0blmC"
      },
      "outputs": [],
      "source": [
        "#Question Type Distribution\n",
        "def is_yes_no(ans):\n",
        "    return isinstance(ans, str) and ans.lower() in [\"yes\", \"no\"]\n",
        "\n",
        "yes_no = sum(is_yes_no(x[\"answer\"]) for x in data)\n",
        "open_ended = len(data) - yes_no\n",
        "\n",
        "print(\"Yes/No questions:\", yes_no)\n",
        "print(\"Open-ended questions:\", open_ended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDvlmjnbpRc"
      },
      "outputs": [],
      "source": [
        "#bar chart\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.bar([\"Yes/No\", \"Open-ended\"], [yes_no, open_ended])\n",
        "plt.title(\"Question Type Distribution\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG4t2_5eodmn"
      },
      "outputs": [],
      "source": [
        "#Question Length\n",
        "question_lengths = [len(x[\"question\"].split()) for x in data]\n",
        "\n",
        "print(\"Avg question length:\", sum(question_lengths)/len(question_lengths))\n",
        "print(\"Min question length:\", min(question_lengths))\n",
        "print(\"Max question length:\", max(question_lengths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmalRMPioe-Y"
      },
      "outputs": [],
      "source": [
        "#Histogram\n",
        "plt.figure()\n",
        "plt.hist(question_lengths, bins=20)\n",
        "plt.title(\"Question Length Distribution\")\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QSnN4MGbpPS"
      },
      "outputs": [],
      "source": [
        "#sample\n",
        "sample = data[0]\n",
        "\n",
        "print(\"Question:\", sample[\"question\"])\n",
        "print(\"Answer:\", sample[\"answer\"])\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open(os.path.join(image_dir, sample[\"image_name\"]))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygxuVM1Wo5V0"
      },
      "outputs": [],
      "source": [
        "#Created the Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: Train (70%) and Temp (30%)\n",
        "train_data, temp_data = train_test_split(\n",
        "    data,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: Validation (15%) and Test (15%)\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_data))\n",
        "print(\"Validation samples:\", len(val_data))\n",
        "print(\"Test samples:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GcOQSrro-oN"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnBPZEPqpDOP"
      },
      "outputs": [],
      "source": [
        "#Define Image Transform\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXDxWXivbtMb"
      },
      "outputs": [],
      "source": [
        "#Check GPU & RAM\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odl2rSDxee9l"
      },
      "outputs": [],
      "source": [
        "#Install Required Libraries\n",
        "!pip install -q transformers accelerate sentencepiece bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhLHpkD3eh08"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmJdevejekpO"
      },
      "outputs": [],
      "source": [
        "# loggin my hugginface\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qAdVB0Femzq"
      },
      "outputs": [],
      "source": [
        "#Load PaliGemma\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"PaliGemma loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq48nDSQep8N"
      },
      "outputs": [],
      "source": [
        "#Prepare a Helper Function\n",
        "def run_paligemma(image_path, question):#Function definition\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\") #Load and prepare the image\n",
        "\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"  #Build the text prompt\n",
        "\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,             #Process image + text together\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,                #Generate the answer\n",
        "        max_new_tokens=30,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    answer = processor.decode(output[0], skip_special_tokens=True)  #Decode tokens back to text\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyA5T5mZeuxq"
      },
      "outputs": [],
      "source": [
        "#Test on a Closed-Ended (Yes/No) Question\n",
        "sample = test_data[0]\n",
        "\n",
        "image_path = os.path.join(image_dir, sample[\"image_name\"])\n",
        "question = sample[\"question\"]\n",
        "gt_answer = sample[\"answer\"]\n",
        "\n",
        "pred_answer = run_paligemma(image_path, question)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Ground Truth:\", gt_answer)\n",
        "print(\"PaliGemma Answer:\", pred_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYP26hTYeyG1"
      },
      "outputs": [],
      "source": [
        "# Find one open-ended question\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() not in [\"yes\", \"no\"]:\n",
        "        sample_oe = item\n",
        "        break\n",
        "\n",
        "image_path = os.path.join(image_dir, sample_oe[\"image_name\"])\n",
        "question = sample_oe[\"question\"]\n",
        "gt_answer = sample_oe[\"answer\"]\n",
        "\n",
        "pred_answer = run_paligemma(image_path, question)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Ground Truth:\", gt_answer)\n",
        "print(\"PaliGemma Answer:\", pred_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oIgwi5oe2Hi"
      },
      "outputs": [],
      "source": [
        "#Helper to clean answers\n",
        "def normalize_yesno(text):\n",
        "    text = text.lower()\n",
        "    if \"yes\" in text:\n",
        "        return \"yes\"\n",
        "    if \"no\" in text:\n",
        "        return \"no\"\n",
        "    return \"unknown\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbO-msXefAQd"
      },
      "outputs": [],
      "source": [
        "#Evaluate on Yes/No Test Set\n",
        "correct = 0\n",
        "total = 0\n",
        "unknown = 0\n",
        "\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() in [\"yes\", \"no\"]:\n",
        "        image_path = os.path.join(image_dir, item[\"image_name\"])\n",
        "        question = item[\"question\"]\n",
        "        gt = item[\"answer\"].lower()\n",
        "\n",
        "        pred = run_paligemma(image_path, question)\n",
        "        pred_norm = normalize_yesno(pred)\n",
        "\n",
        "        if pred_norm == \"unknown\":\n",
        "            unknown += 1\n",
        "        elif pred_norm == gt:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "coverage = 1 - (unknown / total)\n",
        "\n",
        "print(\"Approximate PaliGemma Yes/No Accuracy:\", accuracy)\n",
        "print(\"Answer Coverage (non-unknown):\", coverage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP5_jCmps11U"
      },
      "outputs": [],
      "source": [
        "open_ended_samples = []\n",
        "\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() not in [\"yes\", \"no\"]:\n",
        "        open_ended_samples.append(item)\n",
        "\n",
        "print(\"Total open-ended questions:\", len(open_ended_samples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNgU8fYms3YL"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    sample = open_ended_samples[i]\n",
        "\n",
        "    image_path = os.path.join(image_dir, sample[\"image_name\"])\n",
        "    question = sample[\"question\"]\n",
        "    gt_answer = sample[\"answer\"]\n",
        "\n",
        "    pred_answer = run_paligemma(image_path, question)\n",
        "\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Ground Truth:\", gt_answer)\n",
        "    print(\"PaliGemma Answer:\", pred_answer)\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywqz6MFRsG3K"
      },
      "outputs": [],
      "source": [
        "def keyword_match(gt, pred):\n",
        "    gt = gt.lower()\n",
        "    pred = pred.lower()\n",
        "    return gt in pred or pred in gt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEOXvswVsIIC"
      },
      "outputs": [],
      "source": [
        "matches = 0\n",
        "total = 0\n",
        "\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() not in [\"yes\", \"no\"]:\n",
        "\n",
        "        image_path = os.path.join(image_dir, item[\"image_name\"])\n",
        "        question = item[\"question\"]\n",
        "\n",
        "        pred = run_paligemma(image_path, question)\n",
        "\n",
        "        if keyword_match(item[\"answer\"], pred):\n",
        "            matches += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "score = matches / total\n",
        "print(\"Approximate keyword match rate (open-ended):\", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfx294ZFfGv-"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "\n",
        "for item in test_data:\n",
        "    if (\n",
        "        isinstance(item[\"answer\"], str)\n",
        "        and item[\"answer\"].lower() in [\"yes\", \"no\"]\n",
        "        and count < 3\n",
        "    ):\n",
        "        pred = run_paligemma(\n",
        "            os.path.join(image_dir, item[\"image_name\"]),\n",
        "            item[\"question\"]\n",
        "        )\n",
        "\n",
        "        print(\"Q:\", item[\"question\"])\n",
        "        print(\"GT:\", item[\"answer\"])\n",
        "        print(\"Pred:\", pred)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww-e32RUfO1y"
      },
      "outputs": [],
      "source": [
        "#Install Required Libraries\n",
        "!pip install -q peft datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PVDOGjsfRsY"
      },
      "outputs": [],
      "source": [
        "#Prepare Training Samples\n",
        "def format_sample(sample):\n",
        "    prompt = f\"Question: {sample['question']}\\nAnswer:\"\n",
        "    answer = sample[\"answer\"].lower()\n",
        "    return {\n",
        "        \"image_path\": os.path.join(image_dir, sample[\"image_name\"]),\n",
        "        \"prompt\": prompt,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "train_lora = [format_sample(x) for x in train_data if str(x[\"answer\"]).lower() in [\"yes\", \"no\"]]\n",
        "val_lora   = [format_sample(x) for x in val_data if str(x[\"answer\"]).lower() in [\"yes\", \"no\"]]\n",
        "\n",
        "print(\"LoRA train samples:\", len(train_lora))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R9zYqNOwOhM"
      },
      "outputs": [],
      "source": [
        "#Prepare Training Samples\n",
        "def format_sample(sample):\n",
        "    prompt = f\"Question: {sample['question']}\\nAnswer:\"\n",
        "    answer = str(sample[\"answer\"]).lower()  # ensure string\n",
        "\n",
        "    return {\n",
        "        \"image_path\": os.path.join(image_dir, sample[\"image_name\"]),\n",
        "        \"prompt\": prompt,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "\n",
        "train_lora = [format_sample(x) for x in train_data]\n",
        "val_lora   = [format_sample(x) for x in val_data]\n",
        "\n",
        "\n",
        "print(\"LoRA train samples:\", len(train_lora))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_qqyZ__fb5H"
      },
      "outputs": [],
      "source": [
        "#Create Dataset Class\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class PaliGemmaLoRADataset(Dataset):\n",
        "    def __init__(self, samples, processor, max_length=256):\n",
        "        self.samples = samples\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
        "\n",
        "        # Full text = prompt + answer\n",
        "        full_text = item[\"prompt\"] + \" \" + item[\"answer\"]\n",
        "\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            text=full_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask prompt tokens\n",
        "        prompt_len = len(\n",
        "            self.processor.tokenizer(\n",
        "                item[\"prompt\"],\n",
        "                truncation=True,\n",
        "                max_length=self.max_length\n",
        "            ).input_ids\n",
        "        )\n",
        "\n",
        "        labels[:prompt_len] = -100  # ignore prompt tokens in loss\n",
        "\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        encoding[\"labels\"] = labels\n",
        "\n",
        "        return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0cNBl9sfdE2"
      },
      "outputs": [],
      "source": [
        "#Apply LoRA to PaliGemma\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6YPG1cWfguN"
      },
      "outputs": [],
      "source": [
        "#Training Arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./paligemma_lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrb5iP-2fkrU"
      },
      "outputs": [],
      "source": [
        "train_dataset = PaliGemmaLoRADataset(train_lora, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ2pNNhBfnAu"
      },
      "outputs": [],
      "source": [
        "#Trainer Setup & Training\n",
        "from transformers import Trainer\n",
        "\n",
        "train_dataset = PaliGemmaLoRADataset(train_lora, processor)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-9VUZ8YfqJG"
      },
      "outputs": [],
      "source": [
        "#testing sample\n",
        "sample = test_data[0]\n",
        "pred = run_paligemma(\n",
        "    os.path.join(image_dir, sample[\"image_name\"]),\n",
        "    sample[\"question\"]\n",
        ")\n",
        "\n",
        "print(\"Question:\", sample[\"question\"])\n",
        "print(\"Ground Truth:\", sample[\"answer\"])\n",
        "print(\"After LoRA:\", pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC4egU-kfsyx"
      },
      "outputs": [],
      "source": [
        "#Plot the LoRA Loss Graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "steps = [50,100,150,200,250,300,350,400,450,500,550,600,650,700,750]\n",
        "losses = [\n",
        "    12.8675, 12.4981, 12.4836, 12.4780, 12.4748,\n",
        "    12.4727, 12.4711, 12.4699, 12.4690, 12.4683,\n",
        "    12.4677, 12.4673, 12.4670, 12.4668, 12.4666\n",
        "]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(steps, losses, marker=\"o\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"LoRA Fine-Tuning Loss Curve (PaliGemma)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21bsdlxzfzr1"
      },
      "outputs": [],
      "source": [
        "def run_paligemma_closed(image_path, question):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    prompt = (\n",
        "        \"<image>\\n\"\n",
        "        \"You are a medical assistant.\\n\"\n",
        "        \"Answer using only one word: Yes or No.\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=3,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    decoded = processor.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    if \"yes\" in decoded:\n",
        "        return \"yes\"\n",
        "    if \"no\" in decoded:\n",
        "        return \"no\"\n",
        "    return \"unknown\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YEvKEQwf1Qw"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() in [\"yes\", \"no\"]:\n",
        "        image_path = os.path.join(image_dir, item[\"image_name\"])\n",
        "        question = item[\"question\"]\n",
        "        gt = item[\"answer\"].lower()\n",
        "\n",
        "        pred = run_paligemma_closed(image_path, question)\n",
        "\n",
        "        if pred == gt:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(\"Approximate PaliGemma + LoRA Yes/No Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JQyEU2_8_HR"
      },
      "outputs": [],
      "source": [
        "open_ended_samples = []\n",
        "\n",
        "for item in test_data:\n",
        "    if isinstance(item[\"answer\"], str) and item[\"answer\"].lower() not in [\"yes\", \"no\"]:\n",
        "        open_ended_samples.append(item)\n",
        "\n",
        "print(\"Total open-ended test samples:\", len(open_ended_samples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WREQ8qTc9BYA"
      },
      "outputs": [],
      "source": [
        "num_examples = 3  # you can set 3–5\n",
        "\n",
        "for i in range(num_examples):\n",
        "    sample = open_ended_samples[i]\n",
        "\n",
        "    image_path = os.path.join(image_dir, sample[\"image_name\"])\n",
        "    question = sample[\"question\"]\n",
        "    gt_answer = sample[\"answer\"]\n",
        "\n",
        "    pred_answer = run_paligemma(image_path, question)\n",
        "\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Ground Truth:\", gt_answer)\n",
        "    print(\"Model Answer:\", pred_answer)\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDiEpZA0D6Wc"
      },
      "outputs": [],
      "source": [
        "#Closed-Ended Accuracy Comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\n",
        "    \"CNN-GRU\\n(Trained)\",\n",
        "    \"PaliGemma\\n(Zero-shot)\",\n",
        "    \"PaliGemma + LoRA\"\n",
        "]\n",
        "\n",
        "accuracies = [\n",
        "    0.71,   # CNN-GRU\n",
        "    0.59,   # PaliGemma zero-shot\n",
        "    0.49    # PaliGemma + LoRA (final)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.bar(models, accuracies)\n",
        "plt.ylabel(\"Yes/No Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Closed-Ended (Yes/No) Accuracy Comparison\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcZvxOS4EEr4"
      },
      "outputs": [],
      "source": [
        "#Open-Ended Ability\n",
        "models = [\"CNN-GRU\", \"PaliGemma\", \"PaliGemma + LoRA\"]\n",
        "open_ended_scores = [0.0, 0.21, 0.30]  # approx keyword match / qualitative\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.bar(models, open_ended_scores)\n",
        "plt.ylabel(\"Approx. Open-Ended Match Rate\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Open-Ended Question Performance Comparison\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
